[
  {
    "objectID": "index.html#overview",
    "href": "index.html#overview",
    "title": "Introduction to Metagenomics",
    "section": "Overview",
    "text": "Overview\nMetagenomics is an emerging technique to explore the composition of complex microbial communities, find species or genes of interest in samples with high diversity. This course covers multiple different approaches in metagenomics and covers the bioinformatics analysis part of those.\n\n\n\n\n\n\nLearning Objectives\n\n\n\n\nQuestions you can ask and answers you may expect when using metagenomics approaches\nUnderstand the fundamentals, advantages, disadvantages and potential biases of different approaches\nBe aware of the requirements (knowledge, time, money, hardware) of different approaches\nLearn about different bioinformatics pipelines in metagenimics data analysis\n\n\n\n\nTarget Audience\nThis course was designed for researchers, healthcare and public health professionals with extensive wet-lab but limited bioinformatics experience.\n\n\nPrerequisites\nThe course builds up the knowledge gradually, making it suitable for everyone even without much previous knowledge. However, even if the course includes a short recap session, it is recommended to have a basic understanding of next-generation sequencing techniques, Unix / Linux comman line usage and R / RStudio usage.\n\n\n\nExercises\nExercises in these materials are labelled according to their level of difficulty:\n\n\n\n\n\n\n\nLevel\nDescription\n\n\n\n\n  \nExercises in level 1 are simpler and designed to get you familiar with the concepts and syntax covered in the course.\n\n\n  \nExercises in level 2 combine different concepts together and apply it to a given task.\n\n\n  \nExercises in level 3 require going beyond the concepts and syntax introduced to solve new problems."
  },
  {
    "objectID": "index.html#authors",
    "href": "index.html#authors",
    "title": "Introduction to Metagenomics",
    "section": "Authors",
    "text": "Authors\n\nAbout the authors:\n\nSina Beier \nAffiliation: Staff Bioinformatician, MRC Toxicology, University of Cambridge\nRoles: writing; coding; revision\nRui Guan \nAffiliation: Post-doc Bioinformatician, MRC Toxicology, University of Cambridge\nRoles: writing; coding; revision\nOliver Lorenz \nAffiliation: Bioinformatician, Wellcome Sanger Institute\nRoles: writing; coding\nLajos Kalmar \nAffiliation: Bioinformatics Facility Manager, MRC Toxicology, University of Cambridge\nRoles: writing - original draft; conceptualisation; coding"
  },
  {
    "objectID": "index.html#citation",
    "href": "index.html#citation",
    "title": "Introduction to Metagenomics",
    "section": "Citation",
    "text": "Citation\n\nPlease cite these materials if:\n\nYou adapted or used any of them in your own teaching.\nThese materials were useful for your research work."
  },
  {
    "objectID": "setup.html#data",
    "href": "setup.html#data",
    "title": "2  Data & Setup",
    "section": "Data",
    "text": "Data\nYou can download the data used in the workshop from the following link:\n  Download"
  },
  {
    "objectID": "setup.html#software",
    "href": "setup.html#software",
    "title": "2  Data & Setup",
    "section": "Software",
    "text": "Software\n\nGeneral setup\n\nWindowsMac OSLinux\n\n\n\nSetup the Windows Subsystem for Linux (WSL) following these instructions.\nFrom a WSL terminal (previous step), install Mamba following the Linux instructions on this page.\nInstall R and RStudio following these instructions.\n\n\n\n\nSetup your macOS by following these instructions.\nInstall Mamba using these instructions.\nInstall R and RStudio following these instructions.\n\n\n\n\nInstall Mamba using these instructions.\nInstall R and RStudio following these instructions.\n\n\n\n\n\n\nR Packages\nOpen RStudio. In the R console, run the following commands to install all the necessary packages:\ninstall.packages(\"BiocManager\")\nBiocManager::install(c(\"dada2\", \n                       \"phyloseq\", \n                       \"Biostrings\", \n                       \"ggplot2\", \n                       \"reshape2\", \n                       \"readxl\", \n                       \"tidyverse\"))\n\n\nBioinformatics software\nWe can install the software used in the course using mamba. Due to the large number of programs, we recommend installing them in separate environments to avoid package version conflicts. The following commands install the latest version of each software at the time of writing. You may want to search anaconda.org for the latest versions available.\nmamba create -n alignment fastqc=0.12.1 cutadapt=4.9 trimmomatic=0.39 bowtie2=2.5.4 samtools=1.21 metaphlan=4.1.1 mash=2.3 multiqc==1.25.1\n\nmamba create -n assembly fastqc=0.12.1 cutadapt=4.9 trimmomatic=0.39 bowtie2=2.5.4 samtools=1.21 spades=4.0.0 bbmap=39.10 flash=1.2.11 multiqc==1.25.1\n\nmamba create -n mags maxbin2=2.2.7 prokka=1.14.6 gtdbtk=2.4.0 abricate=1.0.1 checkm-genome=1.2.3\nFrom now on, you can use these packages, by activating the respective software environment using mamba activate alignment, mamba activate assembly or mamba activate mags.\n\n\nDatabases\nSome of the programs used require us to download public databases in addition to their installation. These files can be quite large, so we recommend that you use a shared storage if you’re working in a team. We also recommend that you keep track of the database versions used (e.g. saving them in explicit folder names), in case new updates are released in the future and you want to reproduce an analysis.\n\nCheckM (1.5 GiB)\nFirst activate the environment:\nmamba activate mags\nThe CheckM documentation gives the link to its database file.\nWe will download this databases to a directory in our home called ~/databases/checkmdb_20150116, but you can change this if you prefer to save it elsewhere. We use the date of the latest version of the database in the directory name for reference.\n# create variable with output directory name for our database\n# change this to be a directory of your choice\ncheckm_db=\"$HOME/databases/checkmdb_20150116\"\nmkdir -p $checkm_db\nDownload and decompress the file:\nwget -O checkm_db.tar.gz https://data.ace.uq.edu.au/public/CheckM_databases/checkm_data_2015_01_16.tar.gz\ntar -xzvf checkm_db.tar.gz -C $checkm_db\nrm checkm_db.tar.gz\nAfter downloading, you need to run the following command to configure CheckM:\ncheckm data setRoot $checkm_db\nAlternatively, you can set an environment variable specifically in your Conda/Mamba environment:\nconda env config vars set CHECKM_DATA_PATH=\"$checkm_db\" -n mags\n\n\nGTDB-Tk (40GB)\nThe GTDB-tk documentation gives the link to its database files.\nWe will download this databases to a directory in our home called ~/databases/gtdbtk_r220, but you can change this if you prefer to save it elsewhere. We use the name of the latest version of the database in the directory name for our reference.\n# create variable with output directory name for our database\n# change this to be a directory of your choice\ngtdbtk_db=\"$HOME/databases/gtdbtk_r220\"\nmkdir -p $gtdbtk_db\nDownload and decompress the file:\nwget -O gtdbtk_db.tar.gz https://data.ace.uq.edu.au/public/gtdb/data/releases/latest/auxillary_files/gtdbtk_package/full_package/gtdbtk_data.tar.gz\ntar -xzvf gtdbtk_db.tar.gz -C $gtdbtk_db\nrm checkm_db.tar.gz\nFinally, we need to configure an environment variable to tell GTDB-tk where to find the database. We define this for our Conda/Mamba environment called mags:\nconda env config vars set GTDBTK_DATA_PATH=\"$gtdbtk_db\" -n mags\n\n\nMetaPhlAn (24 GiB)\nMetaPhlAn provides a command to download the latest database from its server (instructions). First we activate our environment and create the directory for the database:\nmamba activate alignment\n\n# create variable with output directory name for our database\n# change this to be a directory of your choice\nmetaphlan_db=\"$HOME/databases/metaphlan\"\nmkdir -p $metaphlan_db\nThen we can run the download command (this can take a long time to finish):\nmetaphlan --install --bowtie2db $metaphlan_db\nFinally, we need to configure an environment variable to tell MetaPhlAn where to find the database. We define this for our Conda/Mamba environment called alignment:\nconda env config vars set DEFAULT_DB_FOLDER=\"$metaphlan_db\" -n alignment"
  },
  {
    "objectID": "materials/11-intro_pres.html#presentation-on-the-fundamentals-of-metagenomics",
    "href": "materials/11-intro_pres.html#presentation-on-the-fundamentals-of-metagenomics",
    "title": "3  Introduction to metagenomics",
    "section": "3.1 Presentation on the fundamentals of metagenomics",
    "text": "3.1 Presentation on the fundamentals of metagenomics\nDuring the opening presentation, you will learn about the main approaches of metagenomics projects. The presentation will provide a brief step-by-step guide on planning and conducting your analysis and disseminating your results.\n\n\n\n\n\n\nLearning Objectives\n\n\n\n\nLearn to choose the most appropriate metagenomics approach\nUnderstand the pros and cons of different methods\nBeing able to plan your own metagenomics project\nRealise that metagenomics is the way forward in your work\nRealise that metagenomics is maybe not the way forward in your work\nBeing able to follow developments on the field"
  },
  {
    "objectID": "materials/11-intro_pres.html#presentation-material",
    "href": "materials/11-intro_pres.html#presentation-material",
    "title": "3  Introduction to metagenomics",
    "section": "3.2 Presentation material",
    "text": "3.2 Presentation material\nThe presentation is accessible on Google Slides on the following link"
  },
  {
    "objectID": "materials/11-intro_pres.html#summary",
    "href": "materials/11-intro_pres.html#summary",
    "title": "3  Introduction to metagenomics",
    "section": "3.3 Summary",
    "text": "3.3 Summary\n\n\n\n\n\n\nKey Points\n\n\n\n\nMetagenomics is a trendy, emerging technique\nIf planned and conducted well it provides significant improvement over traditional microbiology techniques\nIt is proved to be a useful tool in many different areas e.g., in public health, genomics, ecology, environmental science"
  },
  {
    "objectID": "materials/13-16S_seq.html#presentation-on-the-16s-sequencing-technique",
    "href": "materials/13-16S_seq.html#presentation-on-the-16s-sequencing-technique",
    "title": "4  16S sequencing",
    "section": "4.1 Presentation on the 16S sequencing technique",
    "text": "4.1 Presentation on the 16S sequencing technique\nThe most commonly used amplicon based technique in metagenomics is the targeted sequencing of certain variable regions of the 16S ribosomal RNA gene in bacterial genomes. The presentation will give a short introduction to the technology, while the practical session will go through each step of the analysis process. During the practicals, you will learn about the analysis principles and will have deep understanding on the benefits and caveats of the 16S-sequencing technique.\nThe presentation is accessible on Google Slides on the following link.\n\n\n\n\n\n\nKey points\n\n\n\n\n16S amplicon sequencing is the quickest and cheapest method to get community composition information from complex microbial samples.\nSequence context (primer recognition sequence and sequence composition) may introduce abundance bias to the analysis.\nSensitivity is usually better than shotgun metagenomics\nResolution can reach species level but rarely to strain level\nAnalysis can be done in R / RStudio using gold standard pipelines and analysis tools, complemented by extensive reference databases."
  },
  {
    "objectID": "materials/13-16S_seq.html#s-sequencing-practical",
    "href": "materials/13-16S_seq.html#s-sequencing-practical",
    "title": "4  16S sequencing",
    "section": "4.2 16S Sequencing practical",
    "text": "4.2 16S Sequencing practical\nDuring the practical you will analyse the data coming from a pilot study that was aimed to compare two animal facilities (mice faecal samples). Using the results you will be able to investigate the within and between group variance and will be able to infer biologically relevant decisions. The laboratory protocol used PCR primers to amplify the V4 region of the 16S ribosomal RNA gene, the expected length of this region is about 252 base pairs.\n\n4.2.1 Preparing the environment\nThe base R installation is not providing the necessary functions for 16S amplicon sequencing and data analysis, we need to load in the required libraries for our analysis. The dada2 package provides the functions for the data pre-processing and analysis, the Biostrings package helps in sequence data manipulation, the phyloseq package provides functions for post-processing (e.g., composition comparison) and finally we create our plots and graphs with the help of the ggplot2 package. The last line sets the default plot theme to a simple black and white style.\nlibrary(\"dada2\")\nlibrary(\"phyloseq\")\nlibrary(\"Biostrings\")\nlibrary(\"ggplot2\")\ntheme_set(theme_bw())\n\n\n4.2.2 Data import and sample organisation\nIn the first line we define the path to our data folder, followed by reading the list of files for forward and reverse sequencing reads. Finally we extract the sample names from the forward file names.\npath &lt;- \"FIX/ME\"\nfnFs &lt;- sort(list.files(path, pattern=\"_1.fastq.gz\", full.names = TRUE))\nfnRs &lt;- sort(list.files(path, pattern=\"_2.fastq.gz\", full.names = TRUE))\nsample.names &lt;- sapply(strsplit(basename(fnFs), \"_\"), `[`, 1)\n\n\n\n\n\n\nFix the base path\n\n\n\n\n\n\n\nLevel: \nUse the graphical user interface file explorer or the command line to explore the filesystem and find the 16S data folder (00_DATA). Replace the FIX/ME string with the full or relative path to the 00_DATA folder.\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\n\npath &lt;- “~/Course_Materials/00_DATA/”\n\n\n\n\n\n\n\n\n\n\n\n\n4.2.3 Quality control and filtering\nThe first step during the 16S data pre-processing is the inspection of the raw sequencing read qualities and perform trimming and filtering accordingly. The plotQualityProfile() function (from the dada2 package) provides an overview of the read qualities in single or multiple input file(s).\nplotQualityProfile(fnFs[1:3])\nplotQualityProfile(fnRs[1:3])\n\n\n\n\n\n\nPlot 3 random input files\n\n\n\n\n\n\n\nLevel: \nThe provided code will always read in the first three files (as were listed alphabetically in the data folder), selecting the first 3 elements of the fnFs and fnRs vector. Modify the code by utilising the sample() function to select 3 random input file names from the same two vectors. This way you are not constrain your inspection to the same three files, if you re-run the command, you can check a diiferent set of 3 random samples.\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\n\nThe sample() function needs two arguments, a vector (in our case the fnFs and fnRs variables with file names) and the number of random sampled elements.\nplotQualityProfile(sample(fnFs, 3))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nUse the ?plotQualityProfile command in the R console to read more about the function. You will find the detailed description of the output plot in the “Details” section of the help page.\n\n\nThe next commands will define new file names for the filtered raw data and finally perform the data filtering itself. Please use the ?filterAndTrim command to get more information on the filtering function arguments.\nfiltFs &lt;- file.path(path, \"filtered\", paste0(sample.names, \"_1_filt.sequence.txt.gz\"))\nfiltRs &lt;- file.path(path, \"filtered\", paste0(sample.names, \"_2_filt.sequence.txt.gz\"))\nnames(filtFs) &lt;- sample.names\nnames(filtRs) &lt;- sample.names\nout &lt;- filterAndTrim(fnFs, filtFs, fnRs, filtRs, truncLen=c(250,250),\n                     maxN=0, maxEE=c(2,2), truncQ=2, rm.phix=TRUE,\n                     compress=TRUE, multithread=TRUE)\n\n\n\n\n\n\nSet the truncation length\n\n\n\n\n\n\n\nLevel: \nThe truncLen argument of the function defines the lengths (first number for forward, second number for reverse reads) we plan to keep from raw reads. The length has to be set by checking the previously generated quality plots by aiming to keep only high quality regions.\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\n\nThe forward reads are usually high quality along the full sequence, except the last few nucleotides, so we can keep almost the full length (usually we cut 10 nucleotides from the end). The reverse sequencing reads very often have a point where the quality distribution crashes (common in illumina sequencing), you need to cut more off from the end. As an example:\nfilterAndTrim(..., truncLen=c(220,160), ...)\n\n\n\n\n\n\n\n\n\n\nYou can take a quick look at the amount of filtering has been done. If you notice significant drop in the number of reads carried forward, you may want to check the QC graphs and parameters given to the filterAndTrim() function.\nhead(out)\n\n\n4.2.4 Error correction and read merging\nThe DADA2 package uses an adaptive error correction algorithm to learns the error rates introduced by PCR and sequencing. In default DADA2 learns the error rate for each samples individually but to increase sensitivity, pooling and pseudo-pooling of samples is also possible during this step. More information on the pooling techniques can be found on the DADA2 website.\nerrF &lt;- learnErrors(filtFs, multithread=TRUE)  \nerrR &lt;- learnErrors(filtRs, multithread=TRUE)\n\nplotErrors(errF, nominalQ=TRUE)\n\ndadaFs &lt;- dada(filtFs, err=errF, multithread=TRUE)\ndadaRs &lt;- dada(filtRs, err=errR, multithread=TRUE)\nIn the next step we are merging the filtered and denoised forward and reverse reads to construct the full length amplicons. The function merges forward and reverse reads if those are overlapping by at least 12 bases, this length can be fine-tuned using the function’s arguments (please refer to the function’s help page ?mergePairs).\nmergers &lt;- mergePairs(dadaFs, filtFs, dadaRs, filtRs, verbose=TRUE)\n\n# Inspect the merger data.frame from the first sample\nhead(mergers[[1]])\n\n\n4.2.5 Creating and cleaning the ASV table\nThe final step in the DADA2 pipeline is to create the amplicon sequence variant (ASV) table that can be taken forward to more sophisticates analysis. In this table each sequence variant (V4 region with unique sequence) will be counted in each analysed sample, resulting a n x m dimensional table where n is the number of samples and m is the number of observed unique sequence variants.\nseqtab &lt;- makeSequenceTable(mergers)\n\n# Inspect the dimensions of the sequence table\ndim(seqtab)\n\n# Inspect distribution of sequence lengths\ntable(nchar(getSequences(seqtab)))\n\n\n\n\n\n\nTip\n\n\n\nThe sequence length table shows the lengths as headers and the counts for these lengths in the first row. Ideally most of your ASVs should be in the expected range (in the case of the V4 amplicon it is around 252bp), if you see a significant shift to both larger or smaller fragments, you may want to revisit your pre-processing steps.\n\n\nThe ASV table need a final cleanup before it is ready to taken forward to compositional / phylogenic analysis. During the PCR amplification extended primer fragments where the polymerase enzyme didn’t finish the extension on the full length sequence could re-anneal to a different template and result in chimeric sequence variants (the left and the right side of the sequence is coming from different templates). Chimeric sequences are identified if they can be exactly reconstructed by combining a left-segment and a right-segment from two more abundant “parent” sequences.\nseqtab.nochim &lt;- removeBimeraDenovo(seqtab, method=\"consensus\", multithread=TRUE, verbose=TRUE)\n\n#Inspect the dimensions of the final ASV table\ndim(seqtab.nochim)\n\n#Inspect the ratio of merged reads that passed the chimera filtering step\nsum(seqtab.nochim)/sum(seqtab)\n\n\n\n\n\n\nTip\n\n\n\nThe frequency of chimeric sequences varies substantially from dataset to dataset, and depends on on factors including experimental procedures and sample complexity. While it is common that substantial amount of unique ASVs are removed (20-30%), these are usually low abundance ASVs and rarely add up to more than a few percent in terms of the amount of removed merged reads.\n\n\n\n\n4.2.6 Post-pipeline quality control\nOur original raw data went through several pipeline steps where we applied some type of filtering, each time resulting in potential reduction in the number of data points (reads, ASVs). It is a good practice to inspect these reductions across all samples at the end of the pipeline. If we find a significant drop in numbers from one step to the other, that either highlights a general problem with our raw data or wrong choice of parameters (when we see the same drop for all samples) or signals for bad quality on just certain samples (when we only see drops only in a few samples). In both cases it is important to go back either to our laboratory notes or our pipeline and check the steps / samples.\ngetN &lt;- function(x) sum(getUniques(x))\ntrack &lt;- cbind(out, sapply(dadaFs, getN), sapply(dadaRs, getN), sapply(mergers, getN), rowSums(seqtab.nochim))\ncolnames(track) &lt;- c(\"input\", \"filtered\", \"denoisedF\", \"denoisedR\", \"merged\", \"nonchim\")\nrownames(track) &lt;- sample.names\nhead(track)\nThe tabular data can also be visualised by ggplot.\nlibrary(reshape2)\ndf &lt;- melt(track)\nggplot(df, aes(Var2, value, group = Var1, colour = Var1)) + geom_line() + xlab(\"Pipeline step\")\n\n\n4.2.7 Assign taxonomy\nThe discovered 16S V4 fragment sequences are still in plain sequence format in our dataset, we need to identify the ASVs and assign proper taxonomy to those. We are using the Silva database for this purpose.\ntaxa &lt;- assignTaxonomy(seqtab.nochim, \"FIX/ME/silva_nr99_v138.1_wSpecies_train_set.fa.gz\", multithread=TRUE)\n\n\n\n\n\n\nFix the base path\n\n\n\n\n\n\n\nLevel: \nUse the graphical user interface file explorer or the command line to explore the filesystem and find the 16S data folder (00_DATA). Replace the FIX/ME string with the full or relative path to the 00_DATA folder.\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\n\ntaxa &lt;- assignTaxonomy(seqtab.nochim, \"~/Course_Materials/00_DATA/silva_nr99_v138.1_wSpecies_train_set.fa.gz\", multithread=TRUE)\n\n\n\n\n\n\n\n\n\n\n\n\n4.2.8 Visualising results with Phyloseq\nWe will use the Phyloseq package to perform further post processing and visualising differences between the faecal microbiomes from the two animal facilities. First we need to load in a few additional packages to have the appropriate functions for the analysis.\n\nlibrary(\"readxl\")\nlibrary(\"tidyverse\")\nWe are loading in the sample metadata from an external excel file and will use the Sample_ID column to match our DADA2 results with the metadata.\nmetadata &lt;- read_excel(\"FIX/ME/Fecal_Sample_Collection.xlsx\", \n                      sheet = \"Metadata\")\nmetadata &lt;- metadata %&gt;% as.data.frame() %&gt;% column_to_rownames(var = \"Sample_ID\")\nseqtab.nochim &lt;- seqtab.nochim %&gt;% as.data.frame()\n\n\n\n\n\n\nFix the base path\n\n\n\n\n\n\n\nLevel: \nUse the graphical user interface file explorer or the command line to explore the filesystem and find the 16S data folder. Replace the FIX/ME string with the full or relative path to the 00_DATA folder.\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\n\nmetadata &lt;- read_excel(\"~/Course_Materials/00_DATA/Fecal_Sample_Collection.xlsx\", \n                      sheet = \"Metadata\")\n\n\n\n\n\n\n\n\n\n\nIn the next step we will create the phyloseq object, this is a complex variable in R containing all the information needed for further analysis and visualisation. To construct the phyloseq object we need 3 input data: (i) the read count table of the ASVs (abundance data on different species in different samples); (ii) sample metadata (e.g., groups, treatments, disease status); (iii) taxonomic assignments for all ASVs. After creating the object we can use built-in commands to examine the content (e.g., the sample data table).\nps &lt;- phyloseq(otu_table(seqtab.nochim, taxa_are_rows=FALSE), \n               sample_data(metadata), \n               tax_table(taxa))\n\nView(sample_data(ps))\nWe can use the created phyloseq object for many different analysis and visualisations, here we will demonstrate two often used exploration tool, dimensional reduction to see the main microbiome composition structure in all of our samples, and compositional plots for different taxonomic levels.\nps.prop &lt;- transform_sample_counts(ps, function(otu) otu/sum(otu))\nord.nmds.bray &lt;- ordinate(ps.prop, method=\"NMDS\", distance=\"bray\")\nplot_ordination(ps.prop, ord.nmds.bray, color=\"Facility\", title=\"Bray NMDS\")\nThe NMDS (Non-metric Multidimensional Scaling) method reduces the high dimensional complex data (each ASV is a different dimension) to two dimensions. This plot can be used to see the overall similarity / difference between samples, and especially useful to explore within and between groups varibility.\n\n\n\n\n\n\nInvestigate the plot\n\n\n\n\n\n\n\n\nDiscuss the results with the trainers, try to find biological reasons for the difference between the two facilities.\nDiscuss if certain animal experiments would fit better to Facility_1 or Facility_2.\nExplore the distribution of other metadata categories by replacing the Facility word for other column headers in View(sample_data(ps)).\nYou can also try other methods for ordination, refer to the help page of the ordinate() function to see the list of available methods.\n\n\n\n\n\n\nFinally we can plot the composition (on different taxonomical levels) of each sample using bar plots.\ntop20 &lt;- names(sort(taxa_sums(ps), decreasing=TRUE))[1:20]\nps.top20 &lt;- transform_sample_counts(ps, function(OTU) OTU/sum(OTU))\nps.top20 &lt;- prune_taxa(top20, ps.top20)\nplot_bar(ps, fill=\"Phylum\") + facet_grid(~Facility, scales=\"free_x\", space = 'free')\nplot_bar(ps.top20, fill=\"Class\") + facet_grid(~Facility, scales=\"free_x\", space = 'free')\nplot_bar(ps.top20, fill=\"Order\") + facet_grid(~Facility, scales=\"free_x\", space = 'free')"
  },
  {
    "objectID": "materials/21-pres.html#data-formats-qc-and-data-management",
    "href": "materials/21-pres.html#data-formats-qc-and-data-management",
    "title": "5  QC and alignment based metagenomics (theory)",
    "section": "5.1 Data formats, QC and data management",
    "text": "5.1 Data formats, QC and data management\nThe slides for this presentation section can be found on google slides on the following link\n\n5.1.1 File and data formats\nDuring the analysis of shotgun metagenomics data you will work with different data types. Most of these are plain text files (usually compressed by gzip), but having special syntax and internal structure. Understanding how these files look like and especially what purpose they are serving is an important basic knowledge. The most commonly used file formats are listed below, their structure and syntax are detailed in the presentation material.\n\n\n\n\n\n\n\n\nFile type\nFile extension\nMain purpose\n\n\n\n\nFASTA\n.fa, .fasta, .fna\nStoring nucleotide or protein sequence with the sequence identifier (e.g., gene name)\n\n\nFASTQ\n.fastq, .fq, .fastq.gz, .fq.gz\nStoring sequencing raw data, the sequence is stored together with the acquisition quality\n\n\nSAM\n.sam\nStoring alignment data, keeping both the sequence and quality information from the FASTQ file but also storing information on the genomic position that the sequence is aligned to\n\n\nBAM\n.bam\nThe binary compressed format of the SAM file, saves significant amount of storage space compared to SAM file\n\n\nVCF\n.vcf\nStoring genetic variance information based on the position of the reference genome\n\n\n\n\n\n\n\n\n\nKey Points\n\n\n\n\nWhile there are standard tools to handle the above mentioned file types / formats, so most of the time you don’t need to open and inspect them, for troubleshooting purposes it is always good to know more about the format and syntax of these files.\nIf you convert a FASTQ file to a FASTA file (as your application can only read in FASTA format), be prepared that you will loose the sequencing quality information. If you need to do this, it is recommended to perform a quality based filtering first and only convert those sequence records that passed the filter.\nYou can save significant amount of storage space by compressing (by zip or gzip) FASTA, FASTQ and VCF files.\nYou can save significant amount of storage spae if you convert your SAM files to BAM files (they store exactly the same information)\n\n\n\n\n\n5.1.2 Quality control of raw data\nEvery bioinformatics pipeline that processes next-generation sequencing data has to start with quality assessment. There are several measurable properties that users can extract from the raw data (FASTQ files), and these measures can indicate systematic errors that were introduced during sample collection, adapter ligation, library creation or even during the sequencing run. It is important to check all the raw data files, it may turn out that certain problems are only coming up with certain samples or certain batches. FastQC is the most commonly used application for raw data quality assessment.\n\n\n\n\n\n\nTip\n\n\n\nIt is a good practice to archive raw data files with the permanent output file of the FastQC tool (usually a .html report file). This way the quality of the raw data is always available even years later when the data is re-used.\n\n\n\n\n5.1.3 Quality control of the bioinformatics pipeline\nBioinformatics pipelines are diverse and the observed changes, transformations of the data can have different meaning even for the same application in different circumstances. E.g., finding high amount of host DNA contamination during a metagenomics pipeline in faecal samples may raise concerns about the DNA extraction protocol (potential bacterial cell lysis problems), while the similar high amount of host DNA in a metagenomics sample that is coming from swabbing (potentially scraping the host epithelium more) can be normal. Understanding the pipeline steps together with being aware of the potential experimental biases is a very important factor in controlling the quality of a metagenomics pipeline. There are tools to help extracting meaningful information from log files (e.g., MultiQC), but always adapt the “normal ranges” to your own experimental setup.\n\n\n5.1.4 Data management\nManaging data from metagenomics experiments can be a challenge due to the size of raw data, intermediate and processed files. During the planning phase of large metagenimics projects (especially long-term metagenomics based services), it is crucial to calculate with the expenses on data storage, archiving solutions and also on computational capacity.\n\n\n\n\n\n\nKey Points\n\n\n\n\nData in one copy is not data, always keep a copy of your most valuable files (usually the raw data)\nDon’t rely on public repositories as data storage or backup solution. These providers don’t take any responsibility for your data, it serves as a platform for sharing your results with the public, but not as an archiving solution.\nBe aware of your country’s policies and legislation on handling human samples. Even a human faecal sample contains significant amount of host epithelial cells and as a consequence human DNA will be sequenced. Consider “de-humanise” your raw data (remove short reads that are aligning to human genome) in an early step of your pipeline. Depositing metagenomics sample with human DNA information in it is also more complicated.\nDelete intermediate files as soon as you can, keep only those that required a lot of computational resources (so you don’t have to re-do them).\nCompress files to save space, most of the text based formats can be efficiently compressed by the common zip or gzip algorithms. Alignment data should always be stored in BAM files instead of the SAM format."
  },
  {
    "objectID": "materials/21-pres.html#alignment-based-metagenomics",
    "href": "materials/21-pres.html#alignment-based-metagenomics",
    "title": "5  QC and alignment based metagenomics (theory)",
    "section": "5.2 Alignment based metagenomics",
    "text": "5.2 Alignment based metagenomics\nThe slides for this presentation section can be found on google slides on the following link\n\n5.2.1 Fundamentals of alignment based approaches\nIn the alignment based metagenomics approaches we try to find the best matches for our sequencing data among known reference genomes. Match can mean perfect (nucleotide by nuclotide), imperfect (nuclotide changes, insertions, deletions allowed) alignments along the full length of the short read (global alignment) or just within fragments of it (local alignment, hash based methods). From the alignment results, we can derive multiple information: (i) presence / absence of a certain species; (ii) relative abundance of a species (based on alignment coverage data); (iii) genetic distance of a certain species / strain from the closest homologue (based on genetic variations and genome structural homology); (iv) presence / absence of certain genes in a species (e.g., antimicrobial resistance or virulance genes).\n\n\n\n\n\n\nKey Points\n\n\n\n\nYou can only find species / genes with alignment based techniques if you have something similar in your reference database.\nDue to our significantly better knowledge on pathogen species, alignement based methods will always be a bit biased towards well-known species (compared to e.g., commensals in the gut microbiome).\nAlignment based methods are very sensitive and specific to find known genetic material but have severe limitations in discovering emerging, previously unknown species / plasmids / genes.\n\n\n\n\n\n5.2.2 K-mer based profiling\nIn k-mer based methods we randomly extract short nucleotide sequences from reference genomes (fragment size is usually between 16-32 nucleotides, number of fragments ranging from few hundred to several thousands) and screen raw sequencing data (FASTQ files) or assembled contigs (FASTA files) for perfect matching. The theory is, that if we have a certain genome / species in our mixed community high percentage of the fragments, originally extracted from that genome, will show perfect matching with the short reads or the assembled contigs. By defining the sensitivity threshold, we can obtain presence / absence information, by counting an average number of exact matches for all k-mers from the same genome, we can derive abundance information. The most commonly used general k-mer based application is Mash, while a bit more microbiology and metagenomics optimised application is Kraken.\n\n\n\n\n\n\nKey Points\n\n\n\n\nK-mer based methods provide the fastest way to profile shutgun metagenomics raw data. These methods have also relatively low computational resource requirements (CPU, memory).\nAs all alignment based methods, k-mer based methods sensitivity and reliability is highly dependent on our previous knowledge (our initial database).\nWhile species level identification is possible with k-mer based approach, strain specific resolution is challenging.\nAs the selection of k-mers from the reference genomes is random, the presence / absence of specific k-mers rarely have any biological meaning.\n\n\n\n\n\n5.2.3 Marker gene detection based profiling\nAs our knowledge grows in identifying novel microbial genomes, we can compare these genomes and identify genes / gene families that are specific to certain taxonomic levels. This way, we are able to use small subsets of the genome (smaller database, faster bioinformatics pipeline) in the identification of our complex community members. The method provides a biologically meaningful way (as we actually searching for genes) to detect the presence of a previously completely unknown genome and potentially assign to a taxonomic category (even if not identified on species level, we may be able to assign a genus or a family). In this approach the reference gene databases are provided by the software developers (as their generation / modification is not a simple task), so the user has to be very careful of using a well maintained and up-to-date application / database.\n\n\n\n\n\n\nKey Points\n\n\n\n\nMarker gene based methods are usually require slightly more computational resources compared to the k-mer based methods but still have relatively low resource needs (a high-spec desktop or laptop can usually handle the task).\nMarker gene based methods can reliably detect the microbial community members on the species level, further refinement of the gene sequences and polymorphisms (e.g., by using StrainPhlAn) can resolve the genomes in strain level.\n\n\n\n\n\n5.2.4 Whole genome alignment based detection and profiling\nThe most sensitive and specific detection of a certain species or strain of a microbial genome from shotgun metagenomics sequencing can be performed by aligning all raw reads to known whole genomes. While it is almost impossible to use all the known microbial genomes in one step and align reads to them, after a k-mer or marker gene based discovery, the high confidence / abundance genomes can be extracted from the raw data by specifically aligning the reads to the known genomes. Alignment based methods can also be utilised to reduce the amount of raw sequencing reads (for e.g., de novo metagenomics assembly where the amount of reads is in strong positive correlation with the analysis time and memory requirement), highly abundant known genomes and/or the host genome can be used to remove reads that are aligning to these and use the “remaining” raw data to do de novo assembly and novel genome discovery. An extreme example for the usage of alignment based methods is to eliminate certain genomes from a low complexity community (e.g., human blood where we suspect an unknown pathogen), and enriching the raw data for reads originating from the unknown genome.\n\n\n\n\n\n\nKey Points\n\n\n\n\nWhole genome alignment methods in metagenomics rarely used to map the full composition landscape of the community (exceptions are the artificial communities with known components).\nUse full genome alignment approach if you would like to find a specific species / strain with the highest specificity and sensitivity.\nUse the alignment based in combination with a de novo approach to to save computational time and be able to use less resources in the de novo assembly.\nUtilise the whole genome alignment approach to eliminate unwanted and/or well-known genomes from your raw data to make the furhter analysis steps more efficient."
  },
  {
    "objectID": "materials/22-pract.html#compositional-mapping-of-shotgun-metagenomics-data",
    "href": "materials/22-pract.html#compositional-mapping-of-shotgun-metagenomics-data",
    "title": "6  QC k-mer and gene-set based methods",
    "section": "6.1 Compositional mapping of shotgun metagenomics data",
    "text": "6.1 Compositional mapping of shotgun metagenomics data\nWhen we try to describe the composition of more complex communities the potential reference database would be so big using whole genomes (Bacteria + Archea + Fungi + Virus), so we apply different reduction strategies. The two main approaches are:(i) Finding biologically meaningful genetic signatures (e.g., genes that are only present in certain taxonomic group) and search for those in the raw data; (ii) extract numerous random short fragments from genomes and try to find exact matchings for those in the raw data.\n\n6.1.1 MASH, a random k-mer based profiling method\nThere are several bioinformatics applications based on the extraction of multiple k-mers from genomes and then use those to detect the presence of these genomes in raw sequencing or assembled data from mixed microbial communities. The two most popular methods are MASH and Kraken2. K-mers are short (typically 14-30 nucleotides) DNA fragment that are randomly cut out from genomes. The number of k-mer per genome is between a few hundred and a few thousand depending on the required final sensitivity and specificity. If we take an average setting, where we extract 1000 random 21-mer fragments from an average bacterial genome (~5 million basepairs in length), we only cover less than 5% of the genome. While this is a significant reduction in term of information content, it can still be enough to identify species with good sensitivity and specificity. In the following practical, we will demonstrate the usage of mash, we will create our own databases of random k-mers (with different length and number of k-mers) and will also use some generic databases (originally created by using the NCBI RefSeq database).\nFirst, let’s check the different functions of mash, look at their usage and inspect the RefSeq databases.\n\nmash\n\nmash info sg_reference/RefSeqSketches.msh | head -n 20\nmash info sg_reference/RefSeqSketchesDefaults.msh | head -n 20\n\nmash sketch -h\nWe will create a new sketch file from the genomes we used to build up our synthetic community. While this sketch database will have very limited usage, we use this small set of genomes to demonstrate the process of sketching (random k-mer extraction). Doing the same process on meaningful (and potentially very large) databases would take much more time. First we examine the input data, then create a small and ‘light-weight’ database.\n\ncd sg_reference/\n\nless mixed_bacterial_community_ncbi_genomes.fasta\ngrep \"&gt;\" mixed_bacterial_community_ncbi_genomes.fasta\ngrep \"&gt;\" mixed_bacterial_community_ncbi_genomes.fasta | wc -l\n\nmash sketch -i -s 500 -k 16 -o mixed_community mixed_bacterial_community_ncbi_genomes.fasta\n\n# Compare the original fasta file size with the sketch size\nls -lh\n\nmash info mixed_community.msh\n\n\n\n\n\n\nCreate a high resolution sketch database from the same input data\n\n\n\n\n\n\n\nLevel: \nModifying the options of mash sketch by extracting 5000 k-mers with the size of 21 nucleotides, and save it in an output file named mixed_community_hr (referring to high resolution). Compare the size of the two different sketch databases and the original FASTA file, print out the basic information about the new Sketch database. Refer to mash sketch -h if you need information on the different options.\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\n\nmash sketch -i -s 5000 -k 21 -o mixed_community_hr mixed_bacterial_community_ncbi_genomes.fasta\n\nls -lh\n\nmash info mixed_community_hr.msh\n\n\n\n\n\n\n\n\n\n\nIn the next part, you will learn how to profile a mixed community using mash screen. After looking at the general help for the command, you will test a scenario when you have either a full genome or just fragments of it. The second reference sequence (NZ_CP038419_1.fasta) you will screen with mash is not the full genome of the bacteria but only the collection of its genes. While bacterial genomes have low amount of non-coding DNA, the genome is still highly fragmented if each gene is separated to individual fasta records. Not exactly the same way but ending up with highly fragmented genomes in metagenomics studies is very common (especially in de novo assembly). It can happen, that the randomly extracted k-mer in the original genome was in a region that is split into two in the fragmenteg genome. In this case the search algorithm will not find a good match for that k-mer.\n\n\n\n\n\n\nNote\n\n\n\nThe output columns for mash screen are the following:\n\nidentity: level of similarity between the query and the database reference sequence\nshared-hashes: number of matching hashes (k-mers) between the query and the database reference sequence\nmedian-multiplicity\np-value of false detection\nquery-ID of the database entry\nquery-comment of the database entry\n\n\n\nmash screen -h\n\nless NZ_CP034931.fa\ngrep \"&gt;\" NZ_CP034931.fa\n\nless NZ_CP038419_1.fasta\ngrep -c \"&gt;\" _NZ_CP038419_1.fasta\n\nmash screen mixed_community.msh NZ_CP034931.fa\nmash screen mixed_community.msh NZ_CP038419_1.fasta\nNotice, that in both cases not only the query genome came up with high number of shared hashes. This is caused by high similarity between certain genomes but can be avoided by using the -w option. Please read more about this option in the application’s help mash screen -h or on the application’s website. Let’s try running the same commands with the -w option and compare the results\nmash screen -w mixed_community.msh NZ_CP034931.fa\nmash screen mixed_community.msh NZ_CP034931.fa\n\nmash screen -w mixed_community.msh NZ_CP038419_1.fasta\nmash screen mixed_community_hr.msh NZ_CP038419_1.fasta\nIn our previous examples, we used a database that was generated from the genomes we mixed together in the synthetic community against a single genome that we knew was in the synthetic community. This is rarely the case in real life settings, let’s try to screen our raw sequencing data with both our own database and the general RefSeq databases. For the bigger databases we can also try to speed up the process by using multiple CPU cores. Please note that we only print out the 50 best (based on identity level) hist from the big database screens, otherwise the terminal would be flooded with the results.\nmash screen -w mixed_community_hr.msh \\\n../sg_raw_data/mixedcomm_forward_paired.fq.gz \\\n../sg_raw_data/mixedcomm_reverse_paired.fq.gz\n\nmash screen -w RefSeqSketchesDefaults.msh \\\n../sg_raw_data/mixedcomm_forward_paired.fq.gz \\\n../sg_raw_data/mixedcomm_reverse_paired.fq.gz | sort -gr -k 1 | head -n 50\n\nmash screen -w RefSeqSketches.msh \\\n../sg_raw_data/mixedcomm_forward_paired.fq.gz \\\n../sg_raw_data/mixedcomm_reverse_paired.fq.gz | sort -gr -k 1 | head -n 50\n\n\n6.1.2 MetaPhlAn, a marker gene based profiling method\nMetaPhlAn is using a large set of marker genes (~5.1 million) to distinguish between different taxonomic groups (clades). The method can reliably identify genomes down to the species level, further resolution (down to strain level) is possible with the developer’s other algorithm StrainPhlAn. The application uses its own database of selected genes, and due to the large amount of information it uses, it requires a relatively powerful computer to run (at least 6-8 CPU cores and 32GB RAM is highly recommended).The installation and usage is very well documented on the GitHub site (link above).\nWe will do the basic profiling on our pre-processed raw data from the synthetic community and will inspect the output results.\nmetaphlan sg_raw_data/mixedcomm_forward_paired.fq.gz,sg_raw_data/mixedcomm_reverse_paired.fq.gz \\\n--bowtie2out results/metaphlan.bowtie2.bz2 --nproc 5 --input_type fastq -o results/profiled_metagenome.txt\n\nless results/profiled_metagenome.txt\n\n# let's save the species level taxonomic and abundance information for future comparison\ngrep \"s__\" results/profiled_metagenome.txt | grep -v \"t__\" &gt; results/profiled_metagenome_sp_only.txt"
  },
  {
    "objectID": "materials/24-pract.html#finding-a-known-genome-in-mixed-microbial-community",
    "href": "materials/24-pract.html#finding-a-known-genome-in-mixed-microbial-community",
    "title": "7  Whole genome alignment based methods",
    "section": "7.1 Finding a known genome in mixed microbial community",
    "text": "7.1 Finding a known genome in mixed microbial community\nThe following practical simulates the situation when we know what we are looking for and utilise a whole-genome alignment approach to find a particular bacterial strain or virus genome in a highly complex microbial community. This technique is especially useful if the known species (potential pathogen) is hard to detect / culture with traditional laboratory methods, amplification-based methods are not specific enough (e.g., often false positive due to a common species with highly similar genome), the laboratory method requires more time and/or more expensive.\n\n\n\n\n\n\nKey Points to consider\n\n\n\n\nThe closer the genome you use to the known species / strain the higher sensitivity and specificity can be achieved. If you are monitoring an outbreak with this approach, the best results can be achieved if you can isolate the microbe at least once and perform a whole genome sequencing and de novo assembly on it.\nIf you plan to quantify the tracked species, you can have a broad idea (relative abundance) by comparing the aligned read number to the total number of reads in the raw data. You can achieve much better quantification (both relative and absolute) if you spike in your sample (before DNA extraction) with a known bacteria or virus, using a well-defined amount. Ideally the spiked in species should be a distant species in terms of phylogeny and should have similarish genome size.\nIf you don’t have your own reference genome, try to find one in public databases that is potentially the closest to your geographical location but also a recent isolate.\n\n\n\n\n7.1.1 Obtaing and preparing the reference genome for alignment\nReference genomes are commonly stored in FASTA formatted files. If the genome contains multiple DNA species (e.g., mutliple chromosomes or genome + plasmid) it is still stored in a single FASTA file, but in a multi-FASTA format, where each DNA species has its own FASTA header. These genome files can be generated (e.g., by bacterial colony sequencing and de novo assembly) or be downloaded from on-line genome databases. The most commonly used general databases are Ensembl, NCBI GenBank, NCBI RefSeq and the metagenomics specific MGnify.\nThere are multiple reference genome files saved in the Course_Materials/sg_reference/ directory, for this exercise we will use the the reference genome of the Shigella flexneri strain 2013C-3749. We can use the grep command to list the FASTA headers in the file (to see if there are multiple DNA entities), and use the file size (in bytes) for a good estimation of the genome size.\ncd ~/Course_Materials/sg_reference/\n\nls\ngrep \"&gt;\" NZ_CP034931.fa\nls -l NZ_CP034931.fa\nWe will use one of the most popular short sequencing read aligner bowtie2 to align the raw data to the Shigella genome. The aligner requires the genome to be transformed to a binary format that makes the aligner searching significantly better and faster.\nbowtie2-build -q NZ_CP034931.fa shigella_genome\nls\n\n\n\n\n\n\nNote\n\n\n\nPlease note, that the bowtie2-build command resulted in six new files (with the .bt2 extension). For the future alignment run, you will not need the original FASTA file, only these. If you are working with a large size genome (e.g., human genome), you can save space by removing the original FASTA file, in our case the original genome file is small, so it is not necessary to remove it.\n\n\n\n\n7.1.2 Performing the reference based alignment\nOur first command will run the bowtie2 aligner with the default settings, we only need to give the path for the reference genome, the input files (we are only using the trimmed paired-end data) and the output file.\ncd ..\n# be sure, you are in the Course_Materials directory\npwd\nmkdir results\n\nbowtie2 -x sg_reference/shigella_genome \\\n-1 sg_raw_data/mixedcomm_forward_paired.fq.gz \\\n-2 sg_raw_data/mixedcomm_reverse_paired.fq.gz \\\n-S results/shigella_default_alignment.sam\n\n\n\n\n\n\nRe-run the alignment step by utilising a few options\n\n\n\n\n\n\n\nLevel: \nRefer to the above linked website or the command line help (bowtie2 -h) to read about the numerous options the application provides. Re-run the analysis with the following options: - use fast end-to-end alignment - use all the available 8 CPU cores - only output those reads in the final SAM file that have aligned to the reference genome - be sure to use a different output file name\nAfter the run, compare the alignment statistics, running time and the output file size to the previous run.\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\n\nbowtie2 -p 8 --fast --no-unal \\\n-x sg_reference/shigella_genome \\\n-1 sg_raw_data/mixedcomm_forward_paired.fq.gz \\\n-2 sg_raw_data/mixedcomm_reverse_paired.fq.gz \\\n-S results/shigella_fast_alignment.sam\n\n\n\n\n\n\n\n\n\n\nPlease also take your time to read about the different options bowtie2 provides and discuss their potential usage with the trainers.\n\n\n7.1.3 Sort and compress the alignment results\nAs we previously mentioned, while the SAM format is text based and as a result human readable, it is not a good practice to keep you alignment data in SAM format as it can take a lot of storage space. Applications that are used to extract information from alignment data are all capable of reading in and processing the compressed binary file format for alignment data. Most of the downstream applications require the alignment data to be sorted (by alignment position), so we will do the sorting and SAM -&gt; BAM conversion in one step.\ncd results/\n\nsamtools sort -@ 4 -O BAM -o shigella_default_alignment_sorted.bam shigella_default_alignment.sam\nsamtools sort -@ 4 -O BAM -o shigella_fast_alignment_sorted.bam shigella_fast_alignment.sam\n\nls -lh\nrm *.sam\n\n\n\n\n\n\nNote\n\n\n\nPlease note the amount of reduction in file size after the SAM -&gt; BAM conversion. Remember that the BAM format contains all the data that the SAM file contained, so the SAM files can be removed.\n\n\nThe tool we used in this step is another good example for the ‘Swiss army knife’ type applications in bioinformatics. Just like the previously used bowtie2, samtools have a lot of options, running modes (commands) so it can be used for many different purposes. Please take your time to read about the functions on the on-line manual page and discuss the potential usage with the trainers."
  },
  {
    "objectID": "materials/24-pract.html#alignment-pipeline-using-multiple-reference-genomes",
    "href": "materials/24-pract.html#alignment-pipeline-using-multiple-reference-genomes",
    "title": "7  Whole genome alignment based methods",
    "section": "7.2 Alignment pipeline using multiple reference genomes",
    "text": "7.2 Alignment pipeline using multiple reference genomes\nWhile we can use a single genome as a reference and only aiming for detecting the presence / absence of a certain genome in our mixed microbial community, these tools have the potential to provide more information, e.g., relative or absolute abundance. If you are working with a completely unknown sample (e.g., human faecal, or environmental sample) so you don’t know any specific species presence and abundance in your sample, you can ‘spike-in’ a well-defined amount of a lab strain during the sample preparation (ideally before DNA extraction) and compare your detected pathogen genome amounts to that. In the next exercise we are simulating a slightly different scenario, In this case, you are working with a synthetic community, you mix the different species together and would like to describe the relative abundances of the different species.\n\n\n\n\n\n\nNote\n\n\n\nSynthetic communities are often used in the laboratory for different purposes:\n\nStudying or optimising fermentation processes that are involving multiple species\nSimulating gut microbiome with a reduced microbial community and study the changes in response to various effects (antibiotics treatment, pathogen invasion, etc)\nStudying bacterial interactions, metabolic interplay\n\nAs these communities are put together in the lab, the composition in terms of ‘members’ is known, dut metagenomics analysis can be used to finely map the dynamic changes in the community composition (in terms of abundance).\n\n\n\n\n\n\n\n\nAlign your data to multiple genomes\n\n\n\n\n\n\n\nLevel: \nUsing the above single genome alignment as an example, prepare an alignment file for compositional profiling. You will use the same input data (the trimmomatic paired-end output files), and similar pipeline steps, but will use a different reference genome. You can find the reference genome collection (20 different genomes) in the sg_reference/ directory with the file name mixed_bacterial_community_ncbi_genomes.fasta.\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\n\ncd sg_reference/\n\nbowtie2-build -q mixed_bacterial_community_ncbi_genomes.fasta all_genomes\n\ncd ..\n\nbowtie2 -p 8 --fast --no-unal \\\n-x sg_reference/all_genomes \\\n-1 sg_raw_data/mixedcomm_forward_paired.fq.gz \\\n-2 sg_raw_data/mixedcomm_reverse_paired.fq.gz \\\n-S results/all_genomes_fast_alignment.sam\n\ncd results/\n\nsamtools sort -@ 4 -O BAM -o all_genomes_fast_alignment_sorted.bam all_genomes_fast_alignment.sam\n\nrm *.sam\n\n\n\n\n\n\n\n\n\n\n\n7.2.1 Alignment file post-processing\nThe BAM files you have generated are ready to use in multiple applications for further processing or visualisation. Most of these applications require the alignment to be in sorted format (be genome position), that is why we went through the samtools sort step. Certain applications may also require you to ‘index’ the BAM file (to make navigation within the BAM file faster), you can do it with a simple command:\nsamtools index bam_file\n#E.g., in your case\nsamtools index shigella_fast_alignment_sorted.bam\nThis command will create a .bai file next to the BAM file, with the same base name, so applications will find it as the index of the BAM file.\nWe will extract some basic statistics and coverage information from our multiple genome aligned data using various samtools commands.\nsamtools flagstat all_genomes_fast_alignment_sorted.bam\n\nsamtools stats all_genomes_fast_alignment_sorted.bam &gt; alignment_statistics.txt\nless alignment_statistics.txt\n\nsamtools coverage all_genomes_fast_alignment_sorted.bam\n\n# Save this coverage file for future comparison\nsamtools coverage all_genomes_fast_alignment_sorted.bam &gt; mixed_community_coverage_from_alignment.txt\n\n\n\n\n\n\nTip\n\n\n\nBAM files can be used for various other purposes:\n\nVisualisation by Integrated Genome Viewer / IGV or BamView\nVariation calling by using the combination of samtools, bcftools and vcftools\nManipulating data in Python using the pysam package\nManipulating data in R by using the Rsamtools and GenomicAlignments packages"
  },
  {
    "objectID": "materials/23-pract.html#discover-an-unknown-genome-in-low-complexity-mixed-community",
    "href": "materials/23-pract.html#discover-an-unknown-genome-in-low-complexity-mixed-community",
    "title": "8  Finding an unknown genome",
    "section": "8.1 Discover an unknown genome in low complexity mixed community",
    "text": "8.1 Discover an unknown genome in low complexity mixed community\nIn this practical we are simulating a virus infection that is caused by a completely unknown virus. In this simulated dataset a novel genome is hidden in the raw data with a huge background of human genome sequence. This simulates a real scenario when a pathogen is in the blood or other (otherwise sterile) body fluid. To make the bioinformatics step faster we generated the human “background” from chromosome 22, so the database will be relatively small.\n\n\n\n\n\n\nActivate your software environment\n\n\n\nFor this practical we need to activate the software environment called assembly:\nmamba activate assembly\n\n\n\n8.1.1 QC and Pre-processing\nThe raw date quality control and pre-processing is going the same way as we did with the mixed community data, for the details on these steps, please refer to the appropriate practical material.\ncd sg_raw_data/\nfastqc unknown_pathogen_R1.fastq unknown_pathogen_R2.fastq\n\ncutadapt -a CTGTCTCTTATACACATCT -A ATGTGTATAAGAGACA \\\n-o unknown_pathogen_noadapt_R1.fastq -p unknown_pathogen_noadapt_R2.fastq \\\nunknown_pathogen_R1.fastq unknown_pathogen_R2.fastq\n\ntrimmomatic PE -phred33 \\\nunknown_pathogen_noadapt_R1.fastq unknown_pathogen_noadapt_R2.fastq \\\nunknown_forward_paired.fq.gz unknown_forward_unpaired.fq.gz \\\nunknown_reverse_paired.fq.gz unknown_reverse_unpaired.fq.gz \\\nLEADING:3 TRAILING:3 SLIDINGWINDOW:4:15 MINLEN:36\n\n\n8.1.2 Aligning reads to human chromosome 22\nFirst let’s try to align our raw data to the known genome content (in this case the human chromosome 22 sequence). This is again going similarly to our previous practical, first we create a Bowtie2 database, then perform the alignment.\ncd sg_reference/\nbowtie2-build -q Homo_sapiens.GRCh38.dna.chromosome.22.fa human_chr22\n\ncd ..\n\nbowtie2 --qc-filter -p 8 --local -x sg_reference/human_chr22 \\\n-1 sg_raw_data/unknown_forward_paired.fq.gz \\\n-2 sg_raw_data/unknown_reverse_paired.fq.gz \\\n-S results/unknown_pathogen.sam\n\n# Inspect the result .sam file\nless results/unknown_pathogen.sam\nIf we look at the alignment result, we can see the aligning short reads to the human chromosome 22 reference sequence. Reads that are not aligning to the reference genome are also in the SAM file, but in this way not really in a usable format.\n\n\n\n\n\n\nExtract reads that are not aligning to the human chromosome 22 for further processing\n\n\n\n\n\n\n\nLevel: \nRefer to the command line help (bowtie2 -h) to read about the numerous options the application provides. Re-run the bowtie2 alignment step with the following modifications: - use all output fasta files from the trimmomatic output as unpaired reads - Find and use the option that prints out short reads that are not aligning to the reference genome - As we don’t need the alignment SAM file, find a way to “throw out” the SAM file during the alignment - name the output FASTQ file as unknown_enriched.fastq\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\n\nbowtie2 --qc-filter -p 8 --local \\\n-x sg_reference/human_chr22 \\\n-U sg_raw_data/unknown_forward_paired.fq.gz,sg_raw_data/unknown_reverse_paired.fq.gz,sg_raw_data/unknown_forward_unpaired.fq.gz,sg_raw_data/unknown_reverse_unpaired.fq.gz \\\n--un sg_raw_data/unknown_enriched.fastq &gt; /dev/null\nAlternatively, you can concatenate all input FASTQ files into one input file to avoid the long comma separated list.\ncat sg_raw_data/unknown_forward_paired.fq.gz \\\nsg_raw_data/unknown_reverse_paired.fq.gz \\\nsg_raw_data/unknown_forward_unpaired.fq.gz \\\nsg_raw_data/unknown_reverse_unpaired.fq.gz &gt; unknown_unpaired_library.fastq\n\nbowtie2 --qc-filter -p 8 --local \\\n-x sg_reference/human_chr22 \\\n-U unknown_unpaired_library.fastq \\\n--un sg_raw_data/unknown_enriched.fastq &gt; /dev/null\n\n\n\n\n\n\n\n\n\n\n\n\n8.1.3 Assemble the unknown genome\nAfter eliminating the significant amount of human genome background from our raw data, we still only have the short next-generation sequencing reads for our unknown genome. In the next step we will use one of the most popular genome assembler SPAdes, the application that can be used for both single genome and multiple genomes (metagenome) assembly. After looking at the command line help, we will execute the spades.py script with very basic settings (input reads and output folder).\nspades.py\n\n# Run the assembly using all the default settings, only giving the input raw data file and the output folder\nspades.py -s sg_raw_data/unknown_enriched.fastq -o results/unknown_genome\n\n# The assembly results will be in the results/unknown_genome/ folder, check the output files, logs, warnings\ncd results/unknown_genome/\n\n\n\n\n\n\nInvestigate the output of the assembly step\n\n\n\n\n\n\n\nGo through the output files of the spades.py script together with the trainers. Look into the log files, discuss the files that are worth to keep long term. Investigate the level of fragmentation of the assembled novel genome."
  },
  {
    "objectID": "materials/31-pres.html#de-novo-metagenomics-and-hi-c-metagenomics",
    "href": "materials/31-pres.html#de-novo-metagenomics-and-hi-c-metagenomics",
    "title": "9  De novo and Hi-C metagenomics (theory)",
    "section": "9.1 De novo metagenomics and Hi-C metagenomics",
    "text": "9.1 De novo metagenomics and Hi-C metagenomics\nThe slides for this presentation section can be found on google slides on the following link\n\n9.1.1 Fundamentals of de novo and Hi-C metagenomics\nIn the de novo approach of shotgun metagenomics data analysis, we treat the sample as the mixture of completely unknown genomes and use computational and mathematical algorithms to combine raw reads and build contigs (continuous DNA sequences) and then cluster contigs to form assemblies (representing potential individual genomes). Compared to the reference genome based alignment methods (where the composition of our reference genome / gene / kmer set is limiting what we can find), the de novo approach is mostly unbiased, in many cases we have the same chance to assemble a completely unknown commensal and a well-known E. coli.\n\n\n\n\n\n\nDisadvanteges compared to alignment based methods\n\n\n\n\nSensitivity is much lower, low abundance species are much harder to detect and assemble\nSignificantly higher computational resource need, several steps require high amount of CPU cores (&gt;24) and RAM (&gt;100GB). To analyse a few samples with the de novo approach, usually an in-house server is needed, for projects involving large number of samples usually access to high-performance computing (HPC) services is necessary.\nGenetically closely related species are usually hard to separate\nBesides the higher computational resource requirement, this approach requires significantly more time to run the bioinformatics steps and also more advanced bioinformatics knowledge\n\n\n\n\n\n9.1.2 Quality control and pre-processing during de novo analysis\nAs the de novo approach uses the same shotgun sequencing raw data as the alignment based approach, most of the QC and pre-pocessing steps are the same as we discussed in the previous chapter. We use FastQC to check the raw data quality, trimmomatic and cutadapt to trim the raw reads and remove sequencing method introduced extra DNA content. There are two extra pre-processing steps usually performed before we start the next stage and these are both helping / enhancing the main assembly step.First, as the assembly step resource and time need is going up significantly with the amount of input data, it is recommended to remove any duplicates (PCR or optical duplicates) from the raw read data. This is usually performed by the clumpify.sh script from the bbmap software package. Second, the de novo assembly efficiency can be significantly increased (in terms of e.g., reducing the fragmentation of contigs) by introducing longer sequencing reads. Short read sequencing techniques have their length limitations, but we can still make “longer” reads if we merge those read pairs that have overlapping ends (if the sequenced DNA fragment is shorter than the length of the sequencing of the read pair). This step is usually done by the application FLASh obtainable through conda or from the original website.\n\n\n9.1.3 De novo assembly step\nThe pre-processed and quality filtered data is fed into the assembly process. During the de novo assembly the software application fragments the short reads (usually in multiple round into different sized k-mers) and try to find matching fragments. The matching fragments then will be used to find reads that are partially overlapping, so most probably originated from the same template (same genome). The aim of the assembly step is to build up as long continuous DNA fragments (contigs) as possible by combining overlapping short reads. Ideally a whole genome should be built up as a single contig (by reaching the starting point of the contig in circular genomes) but this is very rarely the case. Sequence elements that are found in multiple copies in a single genome or between multiple genomes (intra and inter-genomics repeats) usually break the contig elongation as these repeats have multiple “exit” sequence pathways. The two most popular metagenomics de novo assemblers are metaSPAdes and MEGAHIT.\n\n\n9.1.4 Binning step\nThe result of the de novo assembly step is one big multi-FASTA file containing 10s or often 100s of thousands variable sized contigs. These are all shorter or longer fragments of a genome or a plasmid, but at this point everything is in the same cluster in one single file. The aim of the binning step is to cluster contigs to groups (bins) that are potentially representing individual genomes. This step is very challenging and yet the most unreliable part of the de novo metagenomics pipeline. There are three different approaches we can perform binning with all have their pros and cons.\nSupervised binning, when we use reference genomes to find matching contigs:\n\n(+) Fast, reliable and simple\n(+) Works well for known genomes\n(-) High dependency on prior knowledge\n(-) Knowledge bias can introduce composition bias\n(-) Works poorly for less known microbiomes\n\nUnsupervised binning, when we use genomics properties (e.g., GC content, coverage) to find matching contigs:\n\n(+) No prior knowledge needed\n(+) Good for less studied microbiomes\n(-) Less sensitivity and specificity for known microbiomes\n(-) Usually slow and complex procedure\n(-) Problems separating closely related species\nWorks better with multiple samples in parallel\n\nUsing long read sequencing data to find contigs from the same genome:\n\n(+) No prior knowledge needed\n(+) Can detect segmental rearrangements\n(-) Expensive and requires special laboratory procedures\n(-) Lack of hybrid (short and long read) pipelines and applications\n(-) Method specific issues with accuracy\n\n\n\n9.1.5 Hi-C metagenomics\nPerforming Hi-C metagenomics requires an extra short-read sequencing library besides the standard shotgun metagenomics sequencing. The extra library includes special steps during the laboratory protocol and extra computational steps in the analysi pipeline. The main advantage of the Hi-C metagenomics approach that we create physical links within the cell (cross-links using formaldehyde before cell lysis) between random parts of the genome and between genome and extra-chromosomal DNA (e.g., plasmids). The cross-link information created by the laboratory protocol than will be used during the computational pipeline to help binning (contigs will have physical evidence of belonging to the same cell) and in associating extra chromosomal DNA to the host cell. Identification of the host species for plasmids is a unique feature of Hi-C metagenomics and cannot be achieved by any other metagenomics technique. This makes the Hi-C metagenomics an extremely useful tool in the research / diagnostics / monitoring of antimicrobial resistance presence, transmission and spreading."
  },
  {
    "objectID": "materials/32-pract.html#peforming-de-novo-metagenomics-assembly-on-the-mixed-community-raw-data",
    "href": "materials/32-pract.html#peforming-de-novo-metagenomics-assembly-on-the-mixed-community-raw-data",
    "title": "10  De novo metagenomics assembly",
    "section": "10.1 Peforming de novo metagenomics assembly on the mixed community raw data",
    "text": "10.1 Peforming de novo metagenomics assembly on the mixed community raw data\nIn this practical session we simulate a completely de novo assembly of shotgun sequencing data. We will use the same raw data we used for the alignment based metagenomics practical. As this is a synthetic community we know all the 20 genomes we mixed together in them together with their relative abundance. You can check the species names in the FASTA headers of the sg_reference/mixed_bacterial_community_ncbi_genomes.fasta file and their relative abundancies in the sg_reference/mixed_bacterial_community_5M_abundance.txt file.\n\n10.1.1 QC and pre-processing\nThe de novo assembly QC and pre-processing has the same first steps as any other pipeline on shotgun metagenomics data, as we did these steps already we will skip the FastQC, the cutadapt and the trimmomatic step and carry forward the files we generated during the alignment based metagenomics practical. While it is not crucial, but recommended to perform two extra pre-processing steps when we prepare the data for de novo metagenomcis assembly.\nAs the assembly step time and resource need is correlating significantly with the amount of input data, we can use methods to reduce the amount of raw reads without loosing important data. We will use the clumpify.sh script (from the bbmap package) to remove duplicates (PCR or optical). This algorithm removes completely matching reads or read-pairs.\nclumpify.sh\n\nclumpify.sh in=mixedcomm_forward_paired.fq.gz in2=mixedcomm_reverse_paired.fq.gz \\\nout=mixedcomm_forward_paired_dedup.fq.gz out2=mixedcomm_reverse_paired_dedup.fq.gz \\\ndedupe=t\nIn the next step we will merge overlapping reads (like we did in the 16S pipeline). The main aim of this is to reduce the amount of input sequence and to create longer merged reads that can help during the de novo assembly. We will use the flash application for this purpose.\nflash --help\n\nflash mixedcomm_forward_paired_dedup.fq.gz mixedcomm_reverse_paired_dedup.fq.gz\nls -ltr\nThe script generated 3 files, one for the merged reads (out.extendedFrags.fastq) and 1-1 for the non-merged paired-end reads (out.notCombined_1.fastq and out.notCombined_2.fastq). We will carry forward 3 files for the de novo assembly, two for the paired-end reads and one for unpaired and merged reads. Before we start the assembly we put all the unpaired reads (came from the trimmomatic step and the flash merge) into one file. Be aware that the merged file (from the flash step) is plain FASTQ file, while the unpaired files are gzip-ed, so needed to be unzipped.\nzcat mixedcomm_forward_unpaired.fq.gz mixedcomm_reverse_unpaired.fq.gz &gt;&gt; out.extendedFrags.fastq\n\n\n10.1.2 De novo assembly step\nThe pre-processed data now ready to be fed into the de novo assembler algorithm. We are using the same application (SPAdes) as we used earlier for a single genome assembly, but for this purpose the software package provides a metagenome specific assembler with certain optimisations specific to metagenomics data source. First, look at the command line help of the algorithm, than execute the script with our input data. We only define the number of CPU cores to use together with the 3 input files and the output directory for the results.\n# Perform the de novo assembly step\nmetaspades.py\n\nmetaspades.py -t 8 \\\n-1 out.notCombined_1.fastq \\\n-2 out.notCombined_2.fastq \\\n-s out.extendedFrags.fastq \\\n-o ../results/mixed_comm_de_novo &\n\n\n\n\n\n\nNote\n\n\n\nPlease note that we use the & symbol at the and of the command. This symbol tells the Unix / Linux system to put the run in the background and run the process even if we log out from the server. This is particularly helpful as the de novo assembly usually runs for long time. Even our simulated training data will run for about an hour, a real shotgun metagenomics data can easily provide the amount of raw reads to make the assembly step longer than a day even on a much higher spec computer (e.g., 56 or 67CPU cores)."
  },
  {
    "objectID": "materials/41-pres.html#metagenome-assembled-genomes-mags",
    "href": "materials/41-pres.html#metagenome-assembled-genomes-mags",
    "title": "11  Metagenome assembled genomes (theory)",
    "section": "11.1 Metagenome assembled genomes (MAGs)",
    "text": "11.1 Metagenome assembled genomes (MAGs)\nAfter performing de novo assembly and binning on a shotgun metagenomics data, the results are individual FASTA files with sets of contigs, representing a potential genome. There are multiple inherent errors in binning that is not always easy to avoid or repaer, but at least it is important to discover.\n\n\n\n\n\n\nPotential errors and problems with MAGs\n\n\n\n\nIncomplete genome - most often coming from low abundance and sequence coverage. Due to the lack of appropriate raw sequence coverage the full genome cannot be reconstructed. Very low abundance often leads to no binning at all and completely missing MAG for that species.\nContaminated genome - The MAG contains genomic information from more than one origin (species). The most common cause is the presence of phylogenetically close species with comparable abundance.\nMisplaced mobile genetic elements - traditional shotgun metagenomics sequencing separates the genomic DNA from plasmids at the lysis step, even if the plasmids are perfectly assembled, it is highly unreliable to predict which host they were originally from. Hi-C technique can provide a solution for this problem.\n\n\n\nThe slides for this presentation section can be found on google slides on the following link\n\n11.1.1 Quality control, quality comparison\nThere are multiple solutions to assess the quality of the MAGs after binning. These tools can be used as general QC measures, but can also provide a good platform to compare different metagenomics approaches (e.g., long vs short read sequencing, different DNA extraction methods, different amount of sequencing raw data). While basic information (genome size, number of contigs, longest contigs, coverage info, etc.) can be extracted from the final MAGs by using simple command line commands, more sophisticated measures can be obtained in two fundamentally different ways:\n\nReference genome based quality assessment. In this case we use reference genome databases and try to find the matching reference genomes for our MAGs. After finding the closest reference genome, we can assess the identity and coverage percentage between the genome and the MAG, find missing or surplus genetic material. While this method is highly reliable and have very good specificity for known genomes, it performs poorly in the assessment of novel species. One of the most popular method for reference based assessment is the QUAST package (including MetaQUAST).\nSingle copy gene based quality assessment. Thanks to our existing knowledge on hundreds of thousands prokaryotic genomes, researchers were able to find sets of genes (separately for bacteria and archea) that are existing in all species and existing only in one copy in the genome. While these gene sets are only having around 100 genes, the presence of these genes provide a good proxy to predict the MAG completeness and contamination. If we find all the marker genes in a MAG and we find it only in one copy, we can assume that all the other genes are present in similarly high ratio, so we can assess high completeness and low contamination for the MAG. If we detect less marker genes in the MAG, we can assume a proportionate incompleteness for the whole genome. If we find multiple copies in our MAG for multiple marker genes (that should only be present in single copy) we can assume that the MAG is contaminated (multiple species were binned into one MAG). The most popular applications for single-copy gene based assessment are CheckM and GTDB-tk.\n\n\n\n\n\n\n\nImportant\n\n\n\nIt is not recommended to take contaminated MAGs forward to further analysis (annotation, taxonomic identification, gene search) as the genomic information is likely originated from multiple sources. Incomplete genomes are suitable for further analysis, but the absence of certain genes can be the consequence of the incompleteness.\n\n\n\n\n11.1.2 Identification of genomes and certain gene groups\nEach individual multi-FASTA file (MAG) can be screened in multiple ways to obtain information about its potential taxonomy, specific gene content, or even strain level information.\n\nWe can use the k-mer based methods we used for raw data screening (e.g., mash), those methods work on both raw reads and contigs.\nWe can simply use the NCBI’s BLASTn algorithm, installed locally on our computer or through the web surface.\nFor well-known species (e.g., pathogene bacteria) we can use specialised tools to identify the MAG up to strain level (mlst application).\nTo identify plasmid sequences, we can either use reference based methods (e.g., ABRicate) or sequence composition based methods (e.g., PlasFlow, PlasForest)\nWe can find species or genus specific tools to discover more about the MAG (e.g., legsta for Legionella pneumophila)\nThe GitHub website of Torsten Seemann is an excellent resource for applications in this category.\n\n\n\n11.1.3 De novo genome annotation\nEven if we were able to find out the exact species information for a MAG, but especially if we have find novel genomes (cannot really find a good matching reference genome for it), we can perform de novo gene annotation. During this process we screen for open reading frames (ORFs) and annotate those that are looking like genes. Often, simply based on nucleitide or amino acid homology, we can also annotate the function and the name for the predicted gene. De novo gene annotation tools are generally reliable for archeal and bacterial genomes, it is much more challenging to annotate eukaryotic genomes. The two most popular de novo annotation software for prokaryotic genomes are prokka and bakta. De novo annotation pipelines result in specific file types (typically GFF3 and/or GeneBank format) containing the names, positions, DNA and translated protein sequence of the annotated genes. These files can be taken forward to do comparisons and visualisations.\n\n\n11.1.4 Comparison to other genomes\nNucleotide level whole-genome comparisons can be performed between a MAG and a known genome, or between MAGs (e.g., from different samples or different time points). These comparisons provide high resolution information, even single nucleotide changes can be tracked (e.g., during an outbreak). While we often use known reference genomes for comparisons (e.g., downloaded from NCBI GeneBank or RefSeq), using the laboratory’s own isolates or earlier genome assemblies from the same species often provide higher similarity. These comparisons can be performed with basic tools like BLASTn or high resolution k-mer (high k-mer count and long k-mers) based methods (e.g., using mash).\nWe often like to see our novel MAGs in a broader context. Comparing a genome with multiple isolates/strains from the same species or even with multiple species from the same genus can provide additional and highly valuable information (pan-genome analysis). In this cases the input data is often not the FASTA file with only nucleotide data but the annotated genome(s). After comparing multiple (even hundreds of) genomes we can identify genes that are present in all isolates / MAGs representing the “core genome” in the comparison, and genes that are isolate / MAG / subgroup specific representing the “accessory genome”. These categories can provide information on both isolate / species / genus specific functions and properties (e.g., metabolic capabilities, virulence, etc.). The multi genome comparisons are usually time and resource intensive steps, the two popular applications to perform it are roary and panaroo.\n\n\n\n\n\n\nTip\n\n\n\nWhile using specific options a good level of compatibility can be achieved between de novo annotation and multiple comparison tools, in general prokka works the best in combination with roary while bakta works better with panaroo.\n\n\n\n\n11.1.5 Visualisation tools\nThere is a continuously growing list of visualisation tools for multiple parts of the de novo metagenomics assembly post processing. A few examples from the wide-range of visualisation tools:\n\nGenome annotation visualisation: Artemis, Icarus\nVisualise pan-genome analysis: Phandangoo, PanExplorer, PanACEA\nIntegrated analysis for multiple purposes: MEGAN\nPhylogenic tree visualisation and annotation: iTOL, R packages\nQuality control visualisation: MultiQC\nMAG quality visualisation: QUAST, R/ggplot2"
  },
  {
    "objectID": "materials/42-pract.html#binning-assigning-quality-and-taxonomy-to-mags-annotation",
    "href": "materials/42-pract.html#binning-assigning-quality-and-taxonomy-to-mags-annotation",
    "title": "12  Working with MAGs",
    "section": "12.1 Binning, assigning quality and taxonomy to MAGs, Annotation",
    "text": "12.1 Binning, assigning quality and taxonomy to MAGs, Annotation\nDuring this practical session we will bin the contigs to multiple clusters forming metagenome assembled genomes (MAGs). These MAGs ideally represent individual genomes, we will assign quality measures for these and do de novo gene annotation and specific gene discovery.\n\n\n\n\n\n\nActivate your software environment\n\n\n\nFor this practical we need to activate the software environment called mags:\nmamba activate mags\n\n\n\n12.1.1 Reference independent binning\nWe will use the maxbin2 algorithm to reconstruct the individual genomes of those bacteria that were sequenced in the artificial mixed community. The method uses genomic properties and contig coverage values to find clusters of contigs that are potentially coming from the same source (same genome).\n\n\n\n\n\n\nNote\n\n\n\nDue to the time limitations on this training course we only analyse a single sample, maxbi2 performs significantly better if the binning step is performed on multiple samples in the same time. The multiple samples should come from different sources (e.g., faecal microbiomes from multiple patients), but not from very different sources (e.g., not recommended to bin gut and skin microbiome samples together).\n\n\nFirst let’s just inspect the results of the overnight de novo assembly run.\ncd ~/Course_Materials/\n\ncd results/mixed_comm_de_novo/\nls -lh\nless contigs.fasta\ngrep -c \"&gt;\" contigs.fasta\ngrep \"&gt;\" contigs.fasta | head -n 50\n\n\n\n\n\n\nNote\n\n\n\nPlease note, that while this is a small simulated dataset with only 20 different species mixed together, we still end up with very high number of contigs.\n\n\nLet’s see the application options, create the output folder and run the binning step by giving the input contigs.fasta file, the output directory name, and the raw sequencing data files. The raw data files will be re-aligned to the contigs to calculate precise coverage values.\nrun_MaxBin.pl\nmkdir MAXBIN\n\nrun_MaxBin.pl -thread 8 -contig contigs.fasta -out MAXBIN/mixed_comm \\\n-reads ../../sg_raw_data/out.notCombined_1.fastq \\\n-reads2 ../../sg_raw_data/out.notCombined_2.fastq \\\n-reads3 ../../sg_raw_data/out.extendedFrags.fastq\n\n\n\n\n\n\nInspect the output folder and result files of the maxbin2 run\n\n\n\n\n\n\n\nLevel: \nSwitch to the result directory, check the file sizes and look into the different application result files (not the .fasta files). Discuss the results with the trainers.\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\n\ncd MAXBIN\nls -lh\n\nless mixed_comm.abundance\nless mixed_comm.summary \nless mixed_comm.abund1 \nless mixed_comm.marker\n\n\n\n\n\n\n\n\n\n\n\n\n12.1.2 Assigning quality and taxonomy to the MAGs\nWe will use two different applications (developed by the same research group) checkm and gtdbtk to assign taxonomy and quality to the MAGs. Both methods are using single-copy gene sets to assign quality and their built-in database to assign taxonomy. CheckM is the older but accepted “gold standard” method to assign quality to MAGs. As it was developed using a smaller set of genomes, it is significantly lighter (both in terms of storage space, time and resource need) compared to GTDB-tk.\nWe first run the taxonomy workflow of checkm, followed by the detailed quality assessment.\ncheckm taxonomy_wf domain Bacteria -x fasta MAXBIN/ CHECKM/\n\ncheckm qa CHECKM/Bacteria.ms CHECKM/ --file CHECKM/quality_mixed_comm.tsv --tab_table -o 2\n\n\n\n\n\n\nCheckM result\n\n\n\n\n\n\n\nInspect the result files in the CHECKM/ directory and discuss the assigned MAG quality measures with the trainers. Refer to the software documentation for more information on the usage and output information.\nYou can load in the result .tsv file into a spreadsheet application to make it easier to investigate.\n\n\n\n\n\nThe next step is to run the gtdbtk on the same MAGs. As it was mentioned before, this application uses a much larger reference database, so it also requires more time to run. The next command will run for ~25mins, so it is probably a good time to have a coffee and some biscuits.\n\ngtdbtk classify_wf --out_dir GTDBTK/ --genome_dir MAXBIN/ -x fasta --skip_ani_screen --cpus 8\n\n\n\n\n\n\nGTDB-tk result\n\n\n\n\n\n\n\nInspect the result files in the GTDBTK/ directory and discuss the assigned MAG quality measures with the trainers. Refer to the software documentation for more information on the usage and output information.\nYou can load in the result .tsv file into a spreadsheet application to make it easier to investigate.\n\n\n\n\n\n\n\n12.1.3 De novo gene annotation and screening for AMR genes\nWe will use the prokka application to annotate genes in one of our MAGs. While we will use the most basic way of running the script, prokka has a lot of options for customising the process and results.\nprokka mixed_comm.003.fasta\nIn default prokka outputs the results (annotation files, translated sequences, log files) in a directory named PROKKA_yyyymmdd (with the today’s date). List an check the output files that were generated from the input MAG.\nIn our last example we will use the tool abricate and its built in databases to screen for antimicrobial resistance (AMR) genes. Abricate is a versatile tool that can also search for putative plasmid sequences, virulence genes and AMR genes using different databases. Please refer to the github website to read about all the options, built-in databases and custom database creation.\nabricate mixed_comm.002.fasta\nabricate mixed_comm.006.fasta"
  },
  {
    "objectID": "materials/12-recap_sess.html#presentation-and-demo-on-basic-command-line-usage",
    "href": "materials/12-recap_sess.html#presentation-and-demo-on-basic-command-line-usage",
    "title": "13  Recap session (optional)",
    "section": "13.1 Presentation and demo on basic command line usage",
    "text": "13.1 Presentation and demo on basic command line usage\nDuring the presentation, you will refresh your knowledge on command line usage. You will learn about Unix command line commands that you need to navigate in the filesystem, create and remove files, edit text documents. You will also learn about a conda like package and environment management solution (micromamba) that helps in software installations.\nThe presentation is accessible on Google Slides on the following link.\nA short overview video from the Bioinformatics training Centre Team on Unix command line usage can be found on the following link\n\n\n\n\n\n\nMost important command line commands\n\n\n\n\nls: list folder contents\ntouch: create an empty file (useful to check write permission)\nnano: open text editor\nless: view contents of a file (exit from the viewer by pressing q)\ncp: copy files and folders\nmv: move files and folders\ngrep: search for pattern (e.g., word) in a text file\ncd: change directory\nrm: remove files and folders\ncut: extract sections of a file (e.g., a column of a table)\npwd: print working directory (useful to see the full path)"
  },
  {
    "objectID": "materials/12-recap_sess.html#next-generation-sequencing",
    "href": "materials/12-recap_sess.html#next-generation-sequencing",
    "title": "13  Recap session (optional)",
    "section": "13.2 Next-generation sequencing",
    "text": "13.2 Next-generation sequencing\nDuring the metagenomics data analysis we perform several quality control steps and various types of trimmings and duplication removals. To understand why do you need to do these steps and understand the potential changes / biases you may expect, it is essential to understand the basics of Next-generation sequencing.\nTo refresh your knowledge on Next-generation sequencing techniques, please watch the tutorial video on the following link"
  },
  {
    "objectID": "materials/12-recap_sess.html#summary",
    "href": "materials/12-recap_sess.html#summary",
    "title": "13  Recap session (optional)",
    "section": "13.3 Summary",
    "text": "13.3 Summary\n\n\n\n\n\n\nKey Points\n\n\n\n\nMajority of the tools we use in metagenomics are command line tools, so a good working knowledge in Unix command line is obligatory.\nWhile is is more comfortable to use a text editor with graphical user interface (e.g., Visual Studio Code), the knowledge in pure command line based editor (e.g., nano) will come handy in many occasions.\nA clean and well-organised desktop or office makes the work more efficient, the same way a well-organised computational space makes our metagenomics analysis easier and more efficient. Package managers (like conda, miniconda, mamba, micromamba) combined with virtual environments help efficient software usage.\nMajority of the metagenomics data that we are analysing with bioinformatics tools come from Next-generation sequencing experiments. Knowing the fundamentals of these techniques significantly helps in the usage and understanding the potential benefits and caveats of several steps during the data analysis."
  }
]